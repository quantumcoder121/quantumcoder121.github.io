<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://quantumcoder121.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://quantumcoder121.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-08-18T08:36:44+00:00</updated><id>https://quantumcoder121.github.io/feed.xml</id><title type="html">Aditya Kudre</title><subtitle>A recent IIT Bombay Graduate with deep interest and understanding of Machine Learning and other technologies. </subtitle><entry><title type="html">Probability - Basics</title><link href="https://quantumcoder121.github.io/blog/2024/prob-basics/" rel="alternate" type="text/html" title="Probability - Basics"/><published>2024-08-18T00:00:00+00:00</published><updated>2024-08-18T00:00:00+00:00</updated><id>https://quantumcoder121.github.io/blog/2024/prob-basics</id><content type="html" xml:base="https://quantumcoder121.github.io/blog/2024/prob-basics/"><![CDATA[<h2 id="what-is-probability">What is Probability?</h2> <p>From the name itself, we can deduce that <em>Probability</em> is the chance of something taking place. Essentially, probability measures this chance of something taking place in a numerical manner and quantifies this chance. Now, let’s look at a more formal definition.</p> <h3 id="the-probability-function">The Probability Function</h3> <p>Consider we’re performing an experiment which can have multiple outcomes. We call the set of all such outcomes as our <em>Sample Space</em> \(S\) as any outcome would be “sampled” (or taken) from this set. Further an <em>Event</em> \(E\) is a set of some outcomes of the experiment. Thus, any event \(E\) would be a subset of \(S\) (\(E \subset S\)) and we denote the set of all such subsets as \(F\). Now, a <em>Probability Function</em> \(P[\cdot]\) is a function from \(F\) to \([0, 1]\) that satisfies the following conditions.</p> <ul> <li>\(P[S] = 1\) (Probability of entire sample space is \(1\))</li> <li>\(P[\Phi] = 0\) (\(\Phi\) is the <em>Null Set</em> or the set containing nothing)</li> <li>If two events \(A\) and \(B\) have no common outcomes (i.e. \(A \cap B = \Phi\) or “disjoint” events) then \(P[A \cup B] = P[A] + P[B]\)</li> </ul> <blockquote> <h5 id="note"><em>NOTE</em></h5> <p>More generally, \(F\) is a <a href="https://en.wikipedia.org/wiki/%CE%A3-algebra">\(\sigma\)-field</a> of \(S\)</p> </blockquote> <p>The value of the probability function at an event \(E\) denoted as \(P[E]\) will be the probability of event \(E\) taking place. The more the value of \(P[E]\) the more likely is the event \(E\).</p> <h2 id="some-immediate-results">Some Immediate Results</h2> <p>These are some immediate results that follow from the previous section and we’ll need later.</p> <blockquote> <h5 id="note-1"><em>NOTE</em></h5> <p>Proofs are left as an (optional) exercise for the reader.</p> </blockquote> <ul> <li> <p>\(P[\cup_{i = 1}^{n} A_i] = \Sigma_{i = 1}^{n} P[A_i]\) for disjoint events \(A_i\) (i.e. \(A_i \cap A_j = \Phi\) \(\forall i, j \leq n\) and \(i \ne j\)). This is an extension of the property in previous section to any number of disjoint events and not just two disjoint events.</p> </li> <li> <p>\(P[A] + P[A^{c}] = 1\) where \(A^{c} = S - A\) (set of all elements that are in \(S\) but not in \(A\)). \(P[A^{c}]\) is also the probability of an event \(A\) <strong>not</strong> occuring.</p> </li> <li> <p>\(P[A \cup B] = P[A] + P[B] - P[A \cap B]\) (a particularly useful result)</p> </li> </ul> <h2 id="some-basic-concepts-in-probability">Some Basic Concepts in Probability</h2> <p>Now, we’ll review some basic concepts in probability that would be useful later</p> <h3 id="independence-of-two-events">Independence of Two Events</h3> <blockquote> <h5 id="definition"><em>DEFINITION</em></h5> <p>We say that two events \(A\) and \(B\) are independent (denoted by \(A \perp\!\!\!\perp B\)) if \(P[A \cap B] = P[A] \cdot P[B]\)</p> </blockquote> <h3 id="conditional-probability">Conditional Probability</h3> <p>We denote the (<em>conditional</em>) probability of an event \(A\) occuring given that an event \(B\) has already occured by \(P[A \| B]\). It is clear that</p> \[P[A \| B] = \frac{P[A \cap B]}{P[B]}\] <p>Further, if two events \(A\) and \(B\) are independent, then</p> \[P[A \| B] = \frac{P[A \cap B]}{P[B]} = \frac{P[A] \cdot P[B]}{P[B]} = P[A]\] <p>and similarly, \(P[B \| A] = P[B]\) (note the order of \(A\) and \(B\))</p> <h3 id="conditional-independence">Conditional Independence</h3> <blockquote> <h5 id="definition-1"><em>DEFINITION</em></h5> <p>We say that two events \(A\) and \(B\) are (<em>conditionally</em>) independent given that an event \(C\) has occured (denoted by \((A \perp\!\!\!\perp B) \| C\)) if \(P[(A \cap B) \| C] = P[A \| C] \cdot P[B \| C]\)</p> </blockquote> <h2 id="bayes-theorem">Bayes’ Theorem</h2> <p>Now we come to a very useful result in probability that was discovered by <a href="https://en.wikipedia.org/wiki/Thomas_Bayes">Thomas Bayes</a>. As usual, the proof is left as an (optional) exercise for the reader.</p> <blockquote> <h5 id="thoerem"><em>THOEREM</em></h5> <p>For two events \(A\) and \(B\), \(P[A \| B] = \frac{P[B \| A] \cdot P[A]}{P[B]}\)</p> </blockquote>]]></content><author><name></name></author><category term="math-for-ml"/><category term="probability-primer"/><category term="mathematics"/><category term="probability"/><summary type="html"><![CDATA[Basics of Probability Theory required for Machine Learning]]></summary></entry><entry><title type="html">Machine Learning - an Introduction</title><link href="https://quantumcoder121.github.io/blog/2024/ml-intro/" rel="alternate" type="text/html" title="Machine Learning - an Introduction"/><published>2024-08-17T00:00:00+00:00</published><updated>2024-08-17T00:00:00+00:00</updated><id>https://quantumcoder121.github.io/blog/2024/ml-intro</id><content type="html" xml:base="https://quantumcoder121.github.io/blog/2024/ml-intro/"><![CDATA[<h2 id="what-is-learning">What is Learning?</h2> <p>We start with the most basic of all questions when it comes to Machine Learning. What exactly is <em>Learning</em>? By the term Machine Learning itself, we might understand that it implies that some <em>Machine</em> is learning. What this machine could be is a question we’ll dive into later. But first and foremost, what exactly is <em>Learning</em>?.</p> <p>Consider a <em>Learner</em>. This learner could be an inanimate entity (like a machine) or a person or any other animal. For our discussion, we’ll consider the learner in an abstract sense. This learner is also supposed to do a <em>Task</em> \(T\) and while doing the task \(T\), it is gaining <em>Experience</em> \(E\) on \(T\). We are also measuring its <em>Performance</em> \(P\) on the task \(T\).</p> <blockquote> <h5 id="definition"><em>DEFINITION</em></h5> <p>We say that a learner is <strong>learning</strong> to do a task \(T\) if its performance \(P\) on the task \(T\) increases as it gains experience \(E\) on the task \(T\)</p> </blockquote> <p>In <em>Machine Learning</em> this learner is a <em>Machine</em>, usually a computer or more specifically a CPU/GPU. This is generally how machine learning is defined.</p> <h2 id="the-learner-in-machine-learning">The Learner in Machine Learning</h2> <p>In most of the Machine Learning <em>Algorithms</em> that we’ll study later, the learner is essentially a machine that takes an input and gives out an output.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/ml-intro.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The task of this learner is to correctly match or give out an output when it is provided with an input. We generally calculate its performance based on how correctly it gives out a set of outputs given a set of inputs. The experience is provided in the form of <em>Data</em> (essentially a bunch of information) about the inputs and outputs by a <em>Trainer</em>.</p> <h2 id="supervised-vs-unsupervised-learning">Supervised v/s Unsupervised Learning</h2> <p>The learner that we described in the previous section can be trained in two different ways based on how we provide the data about the inputs and outputs.</p> <p>When we provide <em>Labeled</em> data to the learner (i.e. data with set of inputs and their corresponding correct outputs), we say that the learner is learning in a <em>Supervised</em> manner. As the trainer is supervising the learning process of the learner, we call this Supervised Learning.</p> <p>On the contrary, when we provide <em>Unlabeled</em> data to the learner (i.e. data with set of inputs but not their corresponding correct outputs), we say that the learner is learning in an <em>Unsupervised</em> manner. As the trainer is not supervising the learning process of the learner, we call this Unsupervised Learning.</p> <blockquote> <h5 id="note"><em>NOTE</em></h5> <p>Here, supervision means that the trainer knows beforehand what outputs to expect for some given inputs.</p> </blockquote> <h2 id="online-vs-offline-learning">Online v/s Offline Learning</h2> <p>The data provided to the learner by the trainer can either be in the form of small sets sent over a period of time or it can be sent all at once. The former is known as <em>Online</em> learning while the latter is called as <em>Offline</em> learning.</p> <h2 id="concluding-remarks">Concluding Remarks</h2> <p>That’s all for today’s blog post. We learnt about the basics of machine learning today and also how it is categorized as well as a basic layout for the learner. In tomorrow’s blog post, we’ll learn more about supervised learning</p>]]></content><author><name></name></author><category term="basic-machine-learning"/><category term="machine-learning"/><summary type="html"><![CDATA[A Short Introduction to Machine Learning]]></summary></entry></feed>